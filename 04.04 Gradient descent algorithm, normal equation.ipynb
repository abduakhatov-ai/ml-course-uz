{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Алгоритм градиентного спуска (Gradient Descent Algorithm)\n",
    "Алгоритм градиентного спуска один из наиболее важных алгоритмов в машинном обучении. Как было сказано ранее, подбор наиболее оптимальных параметров модели для конкретного набора данных и называется обучением. Градиентный спуск основной алгоритм, который используется для поиска таких параметров. \n",
    "\n",
    "Алгоритм градиентного спуска является методом оптимизации. С помощью этого алгоритма можно найти такие значения параметров функции, при которых она достигает минимума. В случае машинного обучения необходимо найти точку, при которой функция потерь (loss function) достигает минимума. Найденные параметры будут частью обученной модели, которую затем уже можно использовать для предсказания новых значений целевой переменной.\n",
    "\n",
    "Алгоритм градиентного спуска ищет оптимальные параметры следуя в направлении анти-градиента функции. Градиент это направление наибольшего роста функции из заданной точки. Следовательно анти-градиент это направление, в котором функция уменьшается быстрее всего.\n",
    "\n",
    "Алгоритм работает следующим образом. В начале параметры модели $\\vec{\\theta}$ инициализируются случайным образом. Затем вычисляются значения для каждого из частных производных функции потерь. После этого из каждого параметра вычитается соответствующее вычисленное значение частного производного. Затем процедура повторяется с новыми значениями параметров пока функция потерь не достигнет нуля или не перестанет уменьшатся. Ниже приведен псевдокод алгоритма:\n",
    "```python\n",
    "alpha = 0.05\n",
    "theta = np.random.rand(10)\n",
    "loss_value = calculate_loss(theta)\n",
    "while not enough(loss_value):\n",
    "    grad_values = calc_loss_gradient(theta)\n",
    "    theta = theta - alpha * grad_values\n",
    "```\n",
    "Параметр `alpha` называется скоростью обучения (learning rate). Это так называемый гиперпараметр и используется для настройки алгоритма. Гиперпараметры это такие параметры модели, которые необходимо подобрать для эффективной работы самого алгоритма обучения.\n",
    "\n",
    "На картинке ниже показан принцип алгоритма\n",
    "![gradient_descent](img/gradient_descent.png)\n",
    "\n",
    "Математически алгоритм можно выразить следующим образом. Сначала определим градиент как вектор, элементами которого являются частные производные функции:\n",
    "$$\n",
    "\\Large \\frac{\\partial L(\\vec{\\theta})}{\\partial \\vec{\\theta}} = \\left<\\frac{\\partial L}{\\partial \\theta_0}, \\ldots, \\frac{\\partial L}{\\partial \\theta_n}\\right>\n",
    "$$\n",
    "Затем в цикле будем повторять следующий шаг:\n",
    "$$\n",
    "\\Large\n",
    "\\begin{array}{rcl}\n",
    "temp_0 &:=& \\theta_0 - \\alpha \\frac{\\partial L}{\\partial \\theta_0} \\\\\n",
    "\\vdots \\\\\n",
    "temp_n &:=& \\theta_n - \\alpha \\frac{\\partial L}{\\partial \\theta_n}\\\\\n",
    "\\\\\n",
    "\\theta_0 &:=& temp_0 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n &:=& temp_n\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "В случае линейной регрессии производная для функции потерь будет иметь следующий вид:\n",
    "$$\n",
    "\\Large\n",
    "\\begin{array}{rcl}\n",
    "\\frac{\\partial L(\\vec{\\theta})}{\\partial \\vec{\\theta}} &=& \\frac{1}{2n} \\frac{\\partial(X \\vec{\\theta} - \\vec{y})^2}{\\partial \\vec{\\theta}} \\\\\n",
    "&=& \\frac{1}{n}X^T(X \\vec{\\theta} - \\vec{y})\n",
    "\\end{array}\n",
    "$$\n",
    "Функцию `calc_loss_gradient` для вычисления градиента линейной регрессии в python с помощью NumPy можно реализовать следующим образом:\n",
    "```python\n",
    "def mse_gradient(theta):\n",
    "    y_hat = X.dot(theta)\n",
    "    return X.T.dot(y_hat - y)\n",
    "```\n",
    "\n",
    "## Normal equation\n",
    "Как было сказано ранее, линейная регрессия может быть решена с помощью аналитической функции вместо итеративного алгоритма. Чтобы найти оптимальное значения параметров $\\theta$ можно приравнять градиент нулю и решить линейное уравнения по отношению к $\\theta$:\n",
    "$$\n",
    "\\Large \n",
    "\\begin{array}{rcl} \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\vec{\\theta}} = 0 &\\Leftrightarrow& \\frac{1}{n}X^T(X \\vec{\\theta} - \\vec{y}) = 0\\\\\n",
    "&\\Leftrightarrow& X^T X \\vec{\\theta} -X^T \\vec{y} = 0 \\\\ &\\Leftrightarrow& X^T X \\vec{\\theta} = X^T \\vec{y} \\\\ &\\Leftrightarrow& \\vec{\\theta} = \\left(X^T X\\right)^{-1} X^T \\vec{y} \\end{array}\n",
    "$$\n",
    "Таким образом, чтобы найти оптимальные значения параметров линейной регрессии можно использовать последнее уравнение. Выражение $\\left(X^T X\\right)^{-1}$ называется обратной матрицей. В python с помощью NumPy можно реализовать данное уравнение следующим образом:\n",
    "```python\n",
    "def normal_equation():\n",
    "    inverted = np.linalg.inv(X.T.dot(X))\n",
    "    return inverted.dot(X.T).dot(y)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
